#!/usr/bin/env python3
"""
æ‰¹é‡æ•°æ®æ ‡æ³¨å·¥å…· - ä»æ‰€æœ‰é¢˜åº“æ–‡ä»¶ä¸­æå–æ›´å¤šæ ·æœ¬
"""

import sys
import os
from pathlib import Path
import json
import random
from collections import Counter

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

# ç›´æ¥å¯¼å…¥ï¼Œä¸ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
try:
    from readers import DocumentReader
    from layout_state_machine import LayoutStateMachine
except ImportError:
    # å¤‡ç”¨å¯¼å…¥æ–¹å¼
    sys.path.append(str(Path(__file__).parent / "src" / "io"))
    sys.path.append(str(Path(__file__).parent / "src" / "parsing"))
    from readers import DocumentReader
    from layout_state_machine import LayoutStateMachine

def extract_more_samples():
    """ä»æ‰€æœ‰é¢˜åº“æ–‡ä»¶ä¸­æå–æ›´å¤šæ ·æœ¬"""
    print("ğŸ” æ‰¹é‡æå–é¢˜åº“æ ·æœ¬...")
    
    # æŸ¥æ‰¾æ‰€æœ‰é¢˜åº“æ–‡ä»¶
    tiku_dir = Path("../é¢˜åº“")
    if not tiku_dir.exists():
        tiku_dir = Path(__file__).parent.parent / "é¢˜åº“"
    
    if not tiku_dir.exists():
        print("âŒ é¢˜åº“ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
    supported_files = []
    for pattern in ["*.xlsx", "*.docx"]:  # æš‚æ—¶è·³è¿‡PDF
        supported_files.extend(tiku_dir.glob(pattern))
    
    print(f"ğŸ“ æ‰¾åˆ° {len(supported_files)} ä¸ªé¢˜åº“æ–‡ä»¶")
    
    # è¯»å–å™¨å’Œè§£æå™¨
    reader = DocumentReader()
    parser = LayoutStateMachine()
    
    all_samples = []
    
    for file_path in supported_files:
        print(f"ğŸ“– å¤„ç†æ–‡ä»¶: {file_path.name}")
        
        try:
            # è¯»å–æ–‡æ¡£
            doc_input = reader.read_document(str(file_path))
            
            # è§£æé¢˜ç›®
            questions = parser.parse(doc_input.blocks)
            
            print(f"  ğŸ“Š è§£æåˆ° {len(questions)} ä¸ªé¢˜ç›®")
            
            # éšæœºé‡‡æ ·ï¼ˆé¿å…æ•°æ®è¿‡å¤šï¼‰
            if len(questions) > 100:
                questions = random.sample(questions, 100)
                print(f"  ğŸ² éšæœºé‡‡æ · 100 ä¸ªé¢˜ç›®")
            
            # æ·»åŠ åˆ°æ ·æœ¬é›†
            for i, question in enumerate(questions):
                sample = {
                    "source_id": f"file://{file_path}#q{i+1}",
                    "question": question.question,
                    "options": question.options,
                    "answer_raw": question.answer_raw,
                    "explanation_raw": question.explanation_raw,
                    "predicted_type": "unknown",  # å¾…æ ‡æ³¨
                    "confidence": 0.0,
                    "layout_score": question.layout_score
                }
                all_samples.append(sample)
                
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            continue
    
    print(f"ğŸ“Š æ€»å…±æå– {len(all_samples)} ä¸ªæ ·æœ¬")
    
    # è‡ªåŠ¨æ ‡æ³¨
    auto_labeled = auto_label_samples(all_samples)
    
    # ä¿å­˜æ ·æœ¬
    output_file = Path("data/labels/batch_samples.json")
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(auto_labeled, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ æ ·æœ¬å·²ä¿å­˜è‡³: {output_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    type_counts = Counter(sample['predicted_type'] for sample in auto_labeled)
    print("ğŸ“ˆ æ ‡æ³¨ç»“æœç»Ÿè®¡:")
    for type_name, count in type_counts.items():
        print(f"  {type_name}: {count} ä¸ª")
    
    return auto_labeled

def auto_label_samples(samples):
    """è‡ªåŠ¨æ ‡æ³¨æ ·æœ¬"""
    print("ğŸ¤– å¼€å§‹è‡ªåŠ¨æ ‡æ³¨...")
    
    labeled_samples = []
    
    for sample in samples:
        question = sample['question']
        answer = sample['answer_raw']
        options = sample['options']
        
        # è‡ªåŠ¨æ ‡æ³¨é€»è¾‘
        predicted_type = "unknown"
        confidence = 0.5
        
        # 1. åˆ¤æ–­é¢˜ - æœ€é«˜ä¼˜å…ˆçº§
        if answer and any(marker in answer for marker in ['å¯¹', 'é”™', 'âˆš', 'Ã—', 'True', 'False', 'T', 'F', 'æ­£ç¡®', 'é”™è¯¯']):
            predicted_type = "true_false"
            confidence = 0.9
        
        # 2. å¡«ç©ºé¢˜ - æ£€æµ‹ç©ºç™½æ ‡è®°
        elif ('___' in question or '____' in question or 
              'ï¼ˆ  ï¼‰' in question or '(  )' in question or 
              'ã€  ã€‘' in question):
            predicted_type = "fill_blank"
            confidence = 0.85
        
        # 3. é€‰æ‹©é¢˜ - æœ‰é€‰é¡¹
        elif len(options) >= 2:
            if answer and len(answer) > 1 and all(c in 'ABCDEF' for c in answer):
                predicted_type = "multiple_choice"
                confidence = 0.8
            elif answer and len(answer) == 1 and answer in 'ABCDEF':
                predicted_type = "single_choice"
                confidence = 0.8
            else:
                predicted_type = "single_choice"  # é»˜è®¤å•é€‰
                confidence = 0.6
        
        # 4. ç®€ç­”é¢˜ - å…¶ä»–æƒ…å†µ
        else:
            if any(keyword in question for keyword in ['ç®€è¿°', 'è¯´æ˜', 'è®ºè¿°', 'åˆ†æ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ']):
                predicted_type = "subjective"
                confidence = 0.7
            else:
                predicted_type = "subjective"  # å…œåº•
                confidence = 0.4
        
        sample['predicted_type'] = predicted_type
        sample['confidence'] = confidence
        labeled_samples.append(sample)
    
    return labeled_samples

def convert_to_training_format(samples, target_count=200):
    """è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼å¹¶å¹³è¡¡æ•°æ®"""
    print(f"ğŸ”„ è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ï¼Œç›®æ ‡æ ·æœ¬æ•°: {target_count}")
    
    # æŒ‰ç±»å‹åˆ†ç»„
    by_type = {}
    for sample in samples:
        type_name = sample['predicted_type']
        if type_name not in by_type:
            by_type[type_name] = []
        by_type[type_name].append(sample)
    
    # è®¡ç®—æ¯ä¸ªç±»å‹çš„ç›®æ ‡æ•°é‡ï¼ˆå°½é‡å¹³è¡¡ï¼‰
    num_types = len(by_type)
    target_per_type = target_count // num_types
    
    balanced_samples = []
    
    for type_name, type_samples in by_type.items():
        # å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œå…¨éƒ¨ä½¿ç”¨
        if len(type_samples) <= target_per_type:
            selected = type_samples
        else:
            # éšæœºé‡‡æ ·
            selected = random.sample(type_samples, target_per_type)
        
        print(f"  {type_name}: {len(selected)} ä¸ªæ ·æœ¬")
        balanced_samples.extend(selected)
    
    # è½¬æ¢ä¸ºJSONLæ ¼å¼
    training_samples = []
    for sample in balanced_samples:
        training_sample = {
            "source_id": sample['source_id'],
            "gold_type": sample['predicted_type'],
            "predicted_type": "unknown",
            "confidence": 0.0
        }
        training_samples.append(training_sample)
    
    # ä¿å­˜è®­ç»ƒæ–‡ä»¶
    output_file = Path("data/labels/expanded_labels.jsonl")
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in training_samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    print(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜è‡³: {output_file}")
    print(f"ğŸ“Š æ€»è®¡ {len(training_samples)} ä¸ªå¹³è¡¡æ ·æœ¬")
    
    return training_samples

def main():
    print("ğŸš€ æ‰¹é‡æ•°æ®æ ‡æ³¨å·¥å…·")
    print("=" * 40)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # æå–æ ·æœ¬
    samples = extract_more_samples()
    
    if not samples:
        print("âŒ æ²¡æœ‰æå–åˆ°æ ·æœ¬")
        return
    
    # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
    training_samples = convert_to_training_format(samples, target_count=200)
    
    print("\nğŸ‰ æ‰¹é‡æ ‡æ³¨å®Œæˆï¼")
    print(f"ğŸ“Š å¯ç”¨äºè®­ç»ƒçš„æ ·æœ¬: {len(training_samples)} ä¸ª")
    print("ğŸ¯ ä¸‹ä¸€æ­¥: è¿è¡Œ python train_model.py é‡æ–°è®­ç»ƒæ¨¡å‹")

if __name__ == "__main__":
    main()
