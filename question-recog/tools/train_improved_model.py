#!/usr/bin/env python
"""
æ”¹è¿›çš„XGBoostæ¨¡å‹è®­ç»ƒè„šæœ¬ - ä½¿ç”¨æ‰©å±•æ•°æ®é›†
"""

import sys
import json
import jsonlines
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from collections import Counter

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from features.extractor import FeatureExtractor
from io.schemas import ParsedQuestion

def prepare_expanded_training_data():
    """å‡†å¤‡æ‰©å±•çš„è®­ç»ƒæ•°æ®"""
    print("ğŸ”„ å‡†å¤‡æ‰©å±•è®­ç»ƒæ•°æ®...")
    
    # ä¼˜å…ˆä½¿ç”¨æ‰©å±•æ•°æ®é›†
    expanded_file = Path("data/labels/expanded_labels.jsonl")
    production_file = Path("production_results.json")
    
    if not expanded_file.exists():
        print("âŒ æ‰©å±•æ•°æ®é›†ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ expand_data.py")
        return None, None, None, None
    
    if not production_file.exists():
        print("âŒ ç”Ÿäº§ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ production_test.py")
        return None, None, None, None
    
    # åŠ è½½æ ‡æ³¨æ•°æ®
    labels = {}
    with jsonlines.open(expanded_file) as reader:
        for item in reader:
            labels[item["source_id"]] = item["gold_type"]
    
    print(f"ğŸ“Š åŠ è½½äº† {len(labels)} ä¸ªæ ‡æ³¨æ ·æœ¬")
    
    # åŠ è½½ç”Ÿäº§ç»“æœ
    with open(production_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = data.get("results", [])
    print(f"ğŸ“Š åŠ è½½äº† {len(results)} ä¸ªç”Ÿäº§ç»“æœ")
    
    # åŒ¹é…æ ‡æ³¨å’Œç»“æœ
    feature_extractor = FeatureExtractor()
    X = []
    y = []
    matched_sources = []
    
    # åˆ›å»ºsource_idåˆ°ç»“æœçš„æ˜ å°„
    result_map = {}
    for i, result in enumerate(results):
        source_id = f"production_batch#{i+1}"
        result_map[source_id] = result
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    for source_id, gold_type in labels.items():
        if source_id in result_map:
            result = result_map[source_id]
            question_data = result.get("question", {})
            
            # åˆ›å»ºParsedQuestionå¯¹è±¡
            parsed_q = ParsedQuestion(
                question=question_data.get("question", ""),
                options=question_data.get("options", []),
                answer_raw=question_data.get("answer_raw", ""),
                explanation_raw=question_data.get("explanation_raw", "")
            )
            
            # æå–ç‰¹å¾
            features = feature_extractor.extract_features(parsed_q)
            feature_vector = feature_extractor.features_to_array(features)
            
            X.append(feature_vector)
            y.append(gold_type)
            matched_sources.append(source_id)
    
    print(f"âœ… æˆåŠŸåŒ¹é… {len(X)} ä¸ªæ ·æœ¬")
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    label_counts = Counter(y)
    print("ğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in label_counts.items():
        print(f"  {label:<15}: {count:>3} æ ·æœ¬")
    
    # æ ‡ç­¾ç¼–ç  - å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
    model_dir = Path("src/model")
    model_dir.mkdir(parents=True, exist_ok=True)
    import joblib
    joblib.dump(label_encoder, model_dir / "label_encoder.pkl")
    
    # ä¿å­˜ç‰¹å¾åç§°
    feature_names = feature_extractor.get_feature_names()
    with open(model_dir / "feature_names.json", 'w', encoding='utf-8') as f:
        json.dump(feature_names, f, indent=2, ensure_ascii=False)
    
    return np.array(X), y_encoded, feature_names, label_encoder.classes_

def train_improved_xgboost():
    """è®­ç»ƒæ”¹è¿›çš„XGBoostæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ”¹è¿›çš„XGBoostæ¨¡å‹")
    print("=" * 40)
    
    # å‡†å¤‡æ•°æ®
    X, y, feature_names, class_names = prepare_expanded_training_data()
    
    if X is None:
        print("âŒ æ•°æ®å‡†å¤‡å¤±è´¥")
        return
    
    print(f"ğŸ“Š ç‰¹å¾ç»´åº¦: {X.shape[1]}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {X.shape[0]}")
    print(f"ğŸ“Š ç±»åˆ«æ•°é‡: {len(class_names)}")
    
    # æ•°æ®åˆ†å‰² - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    test_size = min(0.3, max(0.1, len(X) // 10))
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›è¡Œåˆ†å±‚æŠ½æ ·
    label_counts = Counter(y)
    min_count = min(label_counts.values())
    
    if min_count >= 2:
        # å¯ä»¥è¿›è¡Œåˆ†å±‚æŠ½æ ·
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
    else:
        # ä¸èƒ½åˆ†å±‚æŠ½æ ·ï¼Œä½¿ç”¨ç®€å•éšæœºåˆ†å‰²
        print(f"âš ï¸  æ•°æ®ä¸å¹³è¡¡ï¼Œä½¿ç”¨ç®€å•éšæœºåˆ†å‰² (æœ€å°‘ç±»åˆ«åªæœ‰{min_count}ä¸ªæ ·æœ¬)")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # åˆ›å»ºXGBoostæ¨¡å‹ - é’ˆå¯¹å¤šåˆ†ç±»ä¼˜åŒ–
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        n_estimators=300,
        max_depth=6,
        learning_rate=0.08,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        use_label_encoder=False,
        eval_metric='mlogloss'
    )
    
    # äº¤å‰éªŒè¯
    if len(X_train) >= 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œäº¤å‰éªŒè¯
        try:
            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='f1_macro')
            print(f"ğŸ¯ äº¤å‰éªŒè¯ F1 åˆ†æ•°: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        except Exception as e:
            print(f"âš ï¸ äº¤å‰éªŒè¯å¤±è´¥: {e}")
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸ”„ è®­ç»ƒæ¨¡å‹ä¸­...")
    model.fit(X_train, y_train)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred = model.predict(X_test)
    
    # è®¡ç®—F1åˆ†æ•°
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    print(f"âœ… æµ‹è¯•é›† F1 åˆ†æ•° (å®å¹³å‡): {f1_macro:.4f}")
    print(f"âœ… æµ‹è¯•é›† F1 åˆ†æ•° (åŠ æƒå¹³å‡): {f1_weighted:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=class_names))
    
    # æ··æ·†çŸ©é˜µ
    print("\nğŸ” æ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = model.feature_importances_
    important_features = sorted(
        zip(feature_names, feature_importance), 
        key=lambda x: x[1], reverse=True
    )[:10]
    
    print("\nğŸ¯ å‰10ä¸ªé‡è¦ç‰¹å¾:")
    for name, importance in important_features:
        print(f"  {name:<20}: {importance:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    model_dir = Path("src/model")
    model_path = model_dir / "xgb_model.json"
    model.save_model(str(model_path))
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    training_info = {
        "model_type": "XGBoost",
        "training_samples": len(X_train),
        "test_samples": len(X_test),
        "features": len(feature_names),
        "classes": class_names.tolist(),
        "f1_macro": float(f1_macro),
        "f1_weighted": float(f1_weighted),
        "feature_importance": {k: float(v) for k, v in important_features},
        "training_date": "2025-01-12",
        "data_source": "expanded_labels"
    }
    
    info_path = model_dir / "training_info.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(training_info, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ è®­ç»ƒä¿¡æ¯å·²ä¿å­˜è‡³: {info_path}")
    
    # è®­ç»ƒå»ºè®®
    print("\nğŸ’¡ è®­ç»ƒå»ºè®®:")
    if f1_macro < 0.7:
        print("  â€¢ F1åˆ†æ•°è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ›´å¤šæ ‡æ³¨æ•°æ®")
    if len(X_train) < 200:
        print("  â€¢ è®­ç»ƒæ ·æœ¬è¾ƒå°‘ï¼Œå»ºè®®æ ‡æ³¨æ›´å¤šæ•°æ®ä»¥æé«˜æ€§èƒ½")
    if min_count < 10:
        print("  â€¢ æ•°æ®ä¸å¹³è¡¡ä¸¥é‡ï¼Œå»ºè®®å¹³è¡¡å„ç±»åˆ«æ ·æœ¬æ•°é‡")
    
    print("\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("ğŸ¯ ä¸‹ä¸€æ­¥: è¿è¡Œ test_trained_model.py æµ‹è¯•æ¨¡å‹æ•ˆæœ")

if __name__ == "__main__":
    train_improved_xgboost()
