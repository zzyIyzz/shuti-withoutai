#!/usr/bin/env python
"""
ç³»ç»Ÿå…¨é¢å®¡æ ¸å·¥å…· - è¯†åˆ«éœ€è¦äººå·¥é…ç½®çš„é—®é¢˜
"""

import sys
import json
from pathlib import Path

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.io.readers import DocumentReader
from src.pipeline import QuestionRecognitionPipeline
from src.rules.engine import RuleEngine
from src.features.extractor import FeatureExtractor


def comprehensive_system_audit():
    """å…¨é¢ç³»ç»Ÿå®¡æ ¸"""
    print("ğŸ” é¢˜å‹è¯†åˆ«ç³»ç»Ÿå…¨é¢å®¡æ ¸")
    print("=" * 60)
    
    issues = []
    config_needed = []
    
    # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶
    print("ğŸ“‹ 1. é…ç½®æ–‡ä»¶æ£€æŸ¥")
    print("-" * 30)
    
    config_files = [
        "configs/app.yaml",
        "configs/features.yaml", 
        "configs/rules.yaml",
        "configs/logging.yaml"
    ]
    
    for config_file in config_files:
        if Path(config_file).exists():
            print(f"  âœ… {config_file}")
        else:
            print(f"  âŒ {config_file} - ç¼ºå¤±")
            issues.append(f"é…ç½®æ–‡ä»¶ç¼ºå¤±: {config_file}")
    
    # 2. æ£€æŸ¥è§„åˆ™å¼•æ“è¦†ç›–ç‡
    print(f"\nğŸ”§ 2. è§„åˆ™å¼•æ“åˆ†æ")
    print("-" * 30)
    
    engine = RuleEngine()
    rule_stats = engine.get_rule_stats()
    
    print(f"  è§„åˆ™æ€»æ•°: {rule_stats['total_rules']}")
    print(f"  å¯ç”¨è§„åˆ™: {rule_stats['enabled_rules']}")
    
    if rule_stats['enabled_rules'] < rule_stats['total_rules']:
        issues.append("éƒ¨åˆ†è§„åˆ™è¢«ç¦ç”¨ï¼Œå¯èƒ½å½±å“è¯†åˆ«æ•ˆæœ")
    
    # æ£€æŸ¥è§„åˆ™ä¼˜å…ˆçº§
    priorities = rule_stats['rule_priorities']
    print(f"  è§„åˆ™ä¼˜å…ˆçº§:")
    for rule_name, priority in sorted(priorities.items(), key=lambda x: x[1]):
        print(f"    {priority}. {rule_name}")
    
    # 3. æ£€æŸ¥ç‰¹å¾å·¥ç¨‹
    print(f"\nğŸ¯ 3. ç‰¹å¾å·¥ç¨‹åˆ†æ")
    print("-" * 30)
    
    extractor = FeatureExtractor()
    feature_names = extractor.get_feature_names()
    
    print(f"  ç‰¹å¾æ€»æ•°: {len(feature_names)}")
    print(f"  ç‰¹å¾ç±»åˆ«:")
    
    feature_categories = {
        "åŸºç¡€ç‰¹å¾": ["has_options", "num_options", "answer_is_single_letter", "answer_is_multi_letters"],
        "é•¿åº¦ç‰¹å¾": ["question_len", "option_len_mean", "answer_len"],
        "æ ‡ç‚¹ç‰¹å¾": ["punct_density", "question_mark_count"],
        "å…³é”®è¯ç‰¹å¾": ["hint_keywords_multi", "hint_keywords_tf", "hint_keywords_blank", "hint_keywords_subj"],
        "æ¨¡å¼ç‰¹å¾": ["blank_underline_count", "blank_parenthesis_count", "option_alignment_score"],
        "ç‰ˆé¢ç‰¹å¾": ["layout_score", "ocr_conf_mean"],
        "ç­”æ¡ˆç‰¹å¾": ["answer_pattern_id"]
    }
    
    for category, features in feature_categories.items():
        available = sum(1 for f in features if f in feature_names)
        print(f"    {category}: {available}/{len(features)}")
        if available < len(features):
            missing = [f for f in features if f not in feature_names]
            issues.append(f"{category}ç¼ºå¤±ç‰¹å¾: {missing}")
    
    # 4. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    print(f"\nğŸ¤– 4. æ¨¡å‹æ–‡ä»¶æ£€æŸ¥")
    print("-" * 30)
    
    model_files = [
        "src/model/xgb_model.json",
        "src/model/feature_names.json",
        "src/model/training_info.json",
        "calibration/calibration.json",
        "calibration/version.meta"
    ]
    
    model_status = {"trained": False, "calibrated": False}
    
    for model_file in model_files:
        if Path(model_file).exists():
            print(f"  âœ… {model_file}")
            if "xgb_model" in model_file:
                model_status["trained"] = True
            elif "calibration.json" in model_file:
                model_status["calibrated"] = True
        else:
            print(f"  âŒ {model_file} - ç¼ºå¤±")
    
    if not model_status["trained"]:
        config_needed.append("éœ€è¦è®­ç»ƒXGBoostæ¨¡å‹ä»¥æé«˜æœªçŸ¥é¢˜ç›®è¯†åˆ«ç‡")
    
    if not model_status["calibrated"]:
        config_needed.append("éœ€è¦è¿›è¡Œæ¦‚ç‡æ ¡å‡†ä»¥æé«˜ç½®ä¿¡åº¦å‡†ç¡®æ€§")
    
    # 5. æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„
    print(f"\nğŸ“ 5. æ•°æ®ç›®å½•æ£€æŸ¥")
    print("-" * 30)
    
    data_dirs = [
        "data/raw",
        "data/interim", 
        "data/processed",
        "data/labels"
    ]
    
    for data_dir in data_dirs:
        if Path(data_dir).exists():
            file_count = len(list(Path(data_dir).glob("*")))
            print(f"  âœ… {data_dir} ({file_count} æ–‡ä»¶)")
        else:
            print(f"  âŒ {data_dir} - ç¼ºå¤±")
            issues.append(f"æ•°æ®ç›®å½•ç¼ºå¤±: {data_dir}")
    
    # 6. é¢˜åº“è´¨é‡é—®é¢˜åˆ†æ
    print(f"\nğŸ“š 6. é¢˜åº“è´¨é‡é—®é¢˜")
    print("-" * 30)
    
    # åŸºäºä¹‹å‰çš„éªŒè¯ç»“æœ
    quality_issues = [
        "PDFé¢˜åº“è¯†åˆ«ç‡è¿‡ä½ (83.1%æœªçŸ¥)",
        "Wordé¢˜åº“å¯èƒ½å­˜åœ¨æ ¼å¼é—®é¢˜",
        "ç¼ºå°‘æ ‡æ³¨æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒ",
        "è§„åˆ™å¼•æ“è¦†ç›–ç‡ä¸è¶³"
    ]
    
    for issue in quality_issues:
        print(f"  âš ï¸  {issue}")
        issues.append(issue)
    
    # 7. ç”Ÿæˆé…ç½®å»ºè®®
    print(f"\nğŸ’¡ 7. éœ€è¦äººå·¥é…ç½®çš„é¡¹ç›®")
    print("-" * 30)
    
    manual_config_items = [
        {
            "é¡¹ç›®": "è§„åˆ™å¼•æ“ä¼˜åŒ–",
            "æè¿°": "æ ¹æ®å®é™…é¢˜åº“è°ƒæ•´è§„åˆ™ä¼˜å…ˆçº§å’Œé˜ˆå€¼",
            "é…ç½®æ–‡ä»¶": "configs/rules.yaml",
            "ç´§æ€¥ç¨‹åº¦": "é«˜"
        },
        {
            "é¡¹ç›®": "ç‰¹å¾å·¥ç¨‹è°ƒä¼˜",
            "æè¿°": "æ·»åŠ é¢†åŸŸç‰¹å®šçš„å…³é”®è¯å’Œæ¨¡å¼",
            "é…ç½®æ–‡ä»¶": "configs/features.yaml", 
            "ç´§æ€¥ç¨‹åº¦": "ä¸­"
        },
        {
            "é¡¹ç›®": "PDFè§£æä¼˜åŒ–",
            "æè¿°": "è°ƒæ•´PDFæ–‡æœ¬å—åˆå¹¶ç­–ç•¥",
            "é…ç½®æ–‡ä»¶": "configs/app.yaml",
            "ç´§æ€¥ç¨‹åº¦": "é«˜"
        },
        {
            "é¡¹ç›®": "ç½®ä¿¡åº¦é˜ˆå€¼è°ƒæ•´",
            "æè¿°": "æ ¹æ®å®é™…ä½¿ç”¨è°ƒæ•´å„é¢˜å‹çš„æ¥å—/å¤æ ¸é˜ˆå€¼",
            "é…ç½®æ–‡ä»¶": "configs/app.yaml",
            "ç´§æ€¥ç¨‹åº¦": "ä¸­"
        },
        {
            "é¡¹ç›®": "æ•°æ®æ ‡æ³¨",
            "æè¿°": "æ ‡æ³¨100-200ä¸ªæ ·æœ¬ç”¨äºæ¨¡å‹è®­ç»ƒ",
            "é…ç½®æ–‡ä»¶": "data/labels/manual_labels.jsonl",
            "ç´§æ€¥ç¨‹åº¦": "é«˜"
        }
    ]
    
    for item in manual_config_items:
        priority = "ğŸ”´" if item["ç´§æ€¥ç¨‹åº¦"] == "é«˜" else "ğŸŸ¡" if item["ç´§æ€¥ç¨‹åº¦"] == "ä¸­" else "ğŸŸ¢"
        print(f"  {priority} {item['é¡¹ç›®']}")
        print(f"     æè¿°: {item['æè¿°']}")
        print(f"     é…ç½®: {item['é…ç½®æ–‡ä»¶']}")
        print()
    
    # 8. ç”Ÿæˆé—®é¢˜æŠ¥å‘Š
    print(f"\nğŸ“‹ 8. é—®é¢˜æ±‡æ€»")
    print("-" * 30)
    
    print(f"å‘ç°é—®é¢˜: {len(issues)} ä¸ª")
    print(f"éœ€è¦é…ç½®: {len(manual_config_items)} é¡¹")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report = {
        "audit_timestamp": "2025-01-12T15:00:00",
        "issues_found": issues,
        "configuration_needed": manual_config_items,
        "model_status": model_status,
        "recommendations": [
            "ä¼˜å…ˆå¤„ç†PDFè§£æé—®é¢˜ï¼Œè¿™æ˜¯å½“å‰æœ€å¤§çš„ç“¶é¢ˆ",
            "æ ‡æ³¨50-100ä¸ªæ ·æœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒ",
            "è°ƒæ•´è§„åˆ™å¼•æ“ä»¥æé«˜è¦†ç›–ç‡",
            "ä¼˜åŒ–ç½®ä¿¡åº¦é˜ˆå€¼ä»¥å‡å°‘è¯¯åˆ¤"
        ]
    }
    
    with open("system_audit_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: system_audit_report.json")
    
    # 9. ç»™å‡ºç«‹å³è¡ŒåŠ¨å»ºè®®
    print(f"\nğŸš€ ç«‹å³è¡ŒåŠ¨å»ºè®®")
    print("-" * 30)
    
    immediate_actions = [
        "1. è¿è¡Œ annotation_tool.py æ ‡æ³¨50ä¸ªæ ·æœ¬",
        "2. è¿è¡Œ train_model.py è®­ç»ƒXGBoostæ¨¡å‹", 
        "3. è¿è¡Œ calibrate_model.py è¿›è¡Œæ¦‚ç‡æ ¡å‡†",
        "4. è°ƒæ•´ configs/rules.yaml ä¸­çš„è§„åˆ™å‚æ•°",
        "5. ä¼˜åŒ–PDFè§£æçš„æ–‡æœ¬å—åˆå¹¶é€»è¾‘"
    ]
    
    for action in immediate_actions:
        print(f"  {action}")
    
    return report


if __name__ == "__main__":
    comprehensive_system_audit()
