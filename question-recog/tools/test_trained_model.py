#!/usr/bin/env python
"""
æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹æ•ˆæœ
"""

import sys
import json
from pathlib import Path
import numpy as np
import xgboost as xgb

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.pipeline import QuestionRecognitionPipeline
from src.io.readers import DocumentReader


def test_trained_model():
    """æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹æ•ˆæœ")
    print("=" * 40)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = Path("src/model/xgb_model.json")
    if not model_path.exists():
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ train_model.py")
        return
    
    # æ£€æŸ¥è®­ç»ƒä¿¡æ¯
    info_path = Path("src/model/training_info.json")
    if info_path.exists():
        with open(info_path, 'r', encoding='utf-8') as f:
            training_info = json.load(f)
        
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  è®­ç»ƒæ ·æœ¬: {training_info['training_samples']}")
        print(f"  æµ‹è¯•æ ·æœ¬: {training_info['test_samples']}")
        print(f"  F1åˆ†æ•° (å®å¹³å‡): {training_info['f1_macro']:.4f}")
        print(f"  F1åˆ†æ•° (åŠ æƒ): {training_info['f1_weighted']:.4f}")
        print(f"  æ”¯æŒç±»åˆ«: {', '.join(training_info['classes'])}")
    
    # é…ç½®æµæ°´çº¿ï¼ˆæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼‰
    config = {
        "paths": {
            "model_path": str(model_path),
            "calibration_path": "calibration/calibration.json"
        },
        "thresholds": {
            "min_confidence": 0.4,
            "accept": {
                "single_choice": 0.75,
                "multiple_choice": 0.75,
                "true_false": 0.70,
                "fill_blank": 0.65,
                "subjective": 0.60,
            },
            "review": {
                "single_choice": 0.50,
                "multiple_choice": 0.50,
                "true_false": 0.45,
                "fill_blank": 0.40,
                "subjective": 0.35,
            }
        }
    }
    
    # åˆ›å»ºå¸¦æ¨¡å‹çš„æµæ°´çº¿
    pipeline = QuestionRecognitionPipeline(config)
    reader = DocumentReader()
    
    # æµ‹è¯•ä¸€äº›é¢˜ç›®
    print(f"\nğŸ” æµ‹è¯•æ¨¡å‹æ•ˆæœ...")
    
    # æ£€æŸ¥é¢˜åº“ç›®å½•
    tiku_dir = Path("../é¢˜åº“")
    if not tiku_dir.exists():
        print("âŒ é¢˜åº“ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æŸ¥æ‰¾ä¸€ä¸ªé¢˜åº“æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    test_files = list(tiku_dir.glob("*.xlsx"))[:1]  # åªæµ‹è¯•ä¸€ä¸ªæ–‡ä»¶
    
    if not test_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
        return
    
    test_file = test_files[0]
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_file.name}")
    
    try:
        # è¯»å–å¹¶å¤„ç†æ–‡æ¡£
        document = reader.read_document(str(test_file))
        results = pipeline.process_document(document)
        
        # ç»Ÿè®¡ç»“æœ
        stats = {
            "total": len(results),
            "rule_hits": 0,
            "model_predictions": 0,
            "high_confidence": 0,
            "medium_confidence": 0,
            "low_confidence": 0,
            "needs_review": 0,
            "type_distribution": {}
        }
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"æ€»é¢˜ç›®æ•°: {stats['total']}")
        
        # åˆ†ææ¯ä¸ªç»“æœ
        for result in results[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªç»“æœ
            qtype = result.final_result.type.value
            confidence = result.final_result.confidence
            
            # æ›´æ–°ç»Ÿè®¡
            if result.rule_decision:
                stats["rule_hits"] += 1
            else:
                stats["model_predictions"] += 1
            
            if confidence >= 0.8:
                stats["high_confidence"] += 1
            elif confidence >= 0.6:
                stats["medium_confidence"] += 1
            elif confidence >= 0.4:
                stats["low_confidence"] += 1
            
            if result.final_result.needs_review:
                stats["needs_review"] += 1
            
            if qtype not in stats["type_distribution"]:
                stats["type_distribution"][qtype] = 0
            stats["type_distribution"][qtype] += 1
            
            # æ˜¾ç¤ºç»“æœ
            status = "âœ…" if confidence >= 0.7 else "âš ï¸" if confidence >= 0.5 else "âŒ"
            source = "è§„åˆ™" if result.rule_decision else "æ¨¡å‹"
            review_flag = " [éœ€å¤æ ¸]" if result.final_result.needs_review else ""
            
            print(f"  {status} {qtype:15} (ç½®ä¿¡åº¦: {confidence:.3f}) [{source}]{review_flag}")
            print(f"     é¢˜å¹²: {result.question.question[:60]}...")
        
        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
        print(f"è§„åˆ™å‘½ä¸­: {stats['rule_hits']:3} ({stats['rule_hits']/stats['total']*100:5.1f}%)")
        print(f"æ¨¡å‹é¢„æµ‹: {stats['model_predictions']:3} ({stats['model_predictions']/stats['total']*100:5.1f}%)")
        print(f"é«˜ç½®ä¿¡åº¦: {stats['high_confidence']:3} ({stats['high_confidence']/stats['total']*100:5.1f}%)")
        print(f"ä¸­ç½®ä¿¡åº¦: {stats['medium_confidence']:3} ({stats['medium_confidence']/stats['total']*100:5.1f}%)")
        print(f"ä½ç½®ä¿¡åº¦: {stats['low_confidence']:3} ({stats['low_confidence']/stats['total']*100:5.1f}%)")
        print(f"éœ€è¦å¤æ ¸: {stats['needs_review']:3} ({stats['needs_review']/stats['total']*100:5.1f}%)")
        
        print(f"\nğŸ¯ é¢˜å‹åˆ†å¸ƒ:")
        for qtype, count in stats["type_distribution"].items():
            percentage = count / stats['total'] * 100
            print(f"  {qtype:15}: {count:3} é¢˜ ({percentage:5.1f}%)")
        
        # æ€§èƒ½å¯¹æ¯”
        print(f"\nğŸ“Š æ¨¡å‹æ•ˆæœè¯„ä¼°:")
        model_accuracy = (stats['high_confidence'] + stats['medium_confidence']) / stats['total']
        print(f"æ•´ä½“å‡†ç¡®ç‡: {model_accuracy:.1%}")
        
        if stats['model_predictions'] > 0:
            model_contribution = stats['model_predictions'] / stats['total']
            print(f"æ¨¡å‹è´¡çŒ®åº¦: {model_contribution:.1%}")
        
        if stats['needs_review'] / stats['total'] < 0.2:
            print("âœ… éœ€å¤æ ¸æ¯”ä¾‹è¾ƒä½ï¼Œæ¨¡å‹æ•ˆæœè‰¯å¥½")
        else:
            print("âš ï¸ éœ€å¤æ ¸æ¯”ä¾‹è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ•°æ®")
        
        print(f"\nğŸ‰ æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
        
        # ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        if model_accuracy < 0.7:
            print("  â€¢ æ¨¡å‹å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ›´å¤šæ ‡æ³¨æ•°æ®é‡æ–°è®­ç»ƒ")
        if stats['needs_review'] / stats['total'] > 0.3:
            print("  â€¢ éœ€å¤æ ¸æ¯”ä¾‹è¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–è§„åˆ™æˆ–å¢åŠ è®­ç»ƒæ•°æ®")
        if stats['model_predictions'] / stats['total'] < 0.3:
            print("  â€¢ æ¨¡å‹ä½¿ç”¨ç‡è¾ƒä½ï¼Œå¤§éƒ¨åˆ†ç”±è§„åˆ™å¤„ç†ï¼Œå¯ä»¥è€ƒè™‘ä¼˜åŒ–è§„åˆ™è¦†ç›–")
        
        print("  â€¢ è¿è¡Œ calibrate_model.py è¿›è¡Œæ¦‚ç‡æ ¡å‡†")
        print("  â€¢ è¿è¡Œ evaluate_model.py è¿›è¡Œå…¨é¢è¯„ä¼°")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    test_trained_model()
