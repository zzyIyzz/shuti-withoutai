#!/usr/bin/env python
"""
ç®€åŒ–ç‰ˆé¢˜åº“è´¨é‡æ£€æŸ¥å·¥å…· - è‡ªåŠ¨æ£€æŸ¥æ‰€æœ‰é¢˜åº“
"""

import sys
import json
from pathlib import Path
import random

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.io.readers import DocumentReader
from src.pipeline import QuestionRecognitionPipeline


def quick_validate_all():
    """å¿«é€ŸéªŒè¯æ‰€æœ‰é¢˜åº“è´¨é‡"""
    print("ğŸ” è‡ªåŠ¨é¢˜åº“è´¨é‡æ£€æŸ¥")
    print("=" * 50)
    
    # é…ç½®ç³»ç»Ÿ
    config = {
        "thresholds": {
            "min_confidence": 0.4,
            "accept": {
                "single_choice": 0.75,
                "multiple_choice": 0.75,
                "true_false": 0.70,
                "fill_blank": 0.65,
                "subjective": 0.60,
            }
        }
    }
    
    pipeline = QuestionRecognitionPipeline(config)
    reader = DocumentReader()
    
    # æ£€æŸ¥é¢˜åº“ç›®å½•
    tiku_dir = Path("../é¢˜åº“")
    if not tiku_dir.exists():
        print("âŒ é¢˜åº“ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰é¢˜åº“æ–‡ä»¶
    supported_files = []
    for pattern in ["*.xlsx", "*.docx", "*.pdf"]:
        supported_files.extend(tiku_dir.glob(pattern))
    
    if not supported_files:
        print("âŒ æœªæ‰¾åˆ°æ”¯æŒçš„é¢˜åº“æ–‡ä»¶")
        return
    
    print(f"ğŸ“š å‘ç° {len(supported_files)} ä¸ªé¢˜åº“æ–‡ä»¶ï¼Œå¼€å§‹è‡ªåŠ¨æ£€æŸ¥...")
    
    all_reports = []
    
    # æ£€æŸ¥æ¯ä¸ªé¢˜åº“
    for i, file_path in enumerate(supported_files):
        print(f"\n{'='*60}")
        print(f"ğŸ“„ [{i+1}/{len(supported_files)}] æ£€æŸ¥: {file_path.name}")
        print(f"{'='*60}")
        
        try:
            # è¯»å–æ–‡æ¡£
            print("ğŸ”„ è¯»å–æ–‡æ¡£...")
            document = reader.read_document(str(file_path))
            
            # è¯†åˆ«é¢˜å‹
            print("ğŸ¤– è¯†åˆ«é¢˜å‹...")
            results = pipeline.process_document(document)
            
            if not results:
                print("âŒ æœªè¯†åˆ«åˆ°ä»»ä½•é¢˜ç›®")
                continue
            
            # åˆ†æç»“æœ
            print("ğŸ“Š åˆ†æç»“æœ...")
            report = analyze_and_report(results, file_path.name)
            all_reports.append(report)
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            continue
    
    # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
    if all_reports:
        print(f"\n{'='*80}")
        print("ğŸ“‹ æ€»ä½“è´¨é‡æŠ¥å‘Š")
        print(f"{'='*80}")
        generate_summary_report(all_reports)


def analyze_and_report(results, filename):
    """åˆ†æå¹¶ç”ŸæˆæŠ¥å‘Š"""
    total_questions = len(results)
    
    # ç»Ÿè®¡æ•°æ®
    stats = {
        "filename": filename,
        "total_questions": total_questions,
        "type_distribution": {},
        "confidence_levels": {"high": 0, "medium": 0, "low": 0, "very_low": 0},
        "rule_vs_model": {"rule": 0, "model": 0, "unknown": 0},
        "needs_review": 0,
    }
    
    for result in results:
        qtype = result.final_result.type.value
        confidence = result.final_result.confidence
        
        # é¢˜å‹åˆ†å¸ƒ
        if qtype not in stats["type_distribution"]:
            stats["type_distribution"][qtype] = 0
        stats["type_distribution"][qtype] += 1
        
        # ç½®ä¿¡åº¦åˆ†çº§
        if confidence >= 0.8:
            stats["confidence_levels"]["high"] += 1
        elif confidence >= 0.6:
            stats["confidence_levels"]["medium"] += 1
        elif confidence >= 0.4:
            stats["confidence_levels"]["low"] += 1
        else:
            stats["confidence_levels"]["very_low"] += 1
        
        # è§„åˆ™vsæ¨¡å‹
        if result.rule_decision:
            stats["rule_vs_model"]["rule"] += 1
        elif qtype == "unknown":
            stats["rule_vs_model"]["unknown"] += 1
        else:
            stats["rule_vs_model"]["model"] += 1
        
        # éœ€è¦å¤æ ¸
        if result.final_result.needs_review:
            stats["needs_review"] += 1
    
    # æ˜¾ç¤ºæŠ¥å‘Š
    display_quick_report(stats)
    
    # æ˜¾ç¤ºæ ·æœ¬
    show_samples(results, filename)
    
    # è´¨é‡è¯„åˆ†
    score = calculate_quality_score(stats)
    stats["quality_score"] = score
    
    return stats


def display_quick_report(stats):
    """æ˜¾ç¤ºå¿«é€ŸæŠ¥å‘Š"""
    total = stats["total_questions"]
    
    print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»é¢˜ç›®æ•°: {total}")
    
    # é¢˜å‹åˆ†å¸ƒï¼ˆåªæ˜¾ç¤ºå‰5ä¸ªï¼‰
    print(f"ğŸ¯ ä¸»è¦é¢˜å‹:")
    sorted_types = sorted(stats["type_distribution"].items(), key=lambda x: x[1], reverse=True)
    for qtype, count in sorted_types[:5]:
        percentage = count / total * 100
        print(f"  {qtype:15}: {count:4} é¢˜ ({percentage:5.1f}%)")
    
    # ç½®ä¿¡åº¦æ¦‚è§ˆ
    conf = stats["confidence_levels"]
    high_rate = conf["high"] / total * 100
    very_low_rate = conf["very_low"] / total * 100
    
    print(f"ğŸ“ˆ ç½®ä¿¡åº¦æ¦‚è§ˆ:")
    print(f"  é«˜ç½®ä¿¡åº¦ (â‰¥0.8): {conf['high']:4} é¢˜ ({high_rate:5.1f}%)")
    print(f"  æä½ç½®ä¿¡åº¦ (<0.4): {conf['very_low']:4} é¢˜ ({very_low_rate:5.1f}%)")
    
    # éœ€è¦å…³æ³¨çš„é—®é¢˜
    issues = []
    if very_low_rate > 30:
        issues.append(f"âš ï¸  æä½ç½®ä¿¡åº¦é¢˜ç›®è¿‡å¤š ({very_low_rate:.1f}%)")
    if stats["rule_vs_model"]["unknown"] / total > 0.2:
        issues.append(f"âš ï¸  æœªçŸ¥é¢˜å‹è¿‡å¤š ({stats['rule_vs_model']['unknown']/total*100:.1f}%)")
    if stats["needs_review"] / total > 0.3:
        issues.append(f"âš ï¸  éœ€å¤æ ¸é¢˜ç›®è¿‡å¤š ({stats['needs_review']/total*100:.1f}%)")
    
    if issues:
        print(f"ğŸš¨ éœ€è¦å…³æ³¨:")
        for issue in issues:
            print(f"  {issue}")
    else:
        print(f"âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")


def show_samples(results, filename):
    """æ˜¾ç¤ºæ ·æœ¬é¢˜ç›®"""
    print(f"\nğŸ” æ ·æœ¬é¢˜ç›®:")
    
    # æŒ‰é¢˜å‹åˆ†ç»„
    by_type = {}
    for result in results:
        qtype = result.final_result.type.value
        if qtype not in by_type:
            by_type[qtype] = []
        by_type[qtype].append(result)
    
    # æ˜¾ç¤ºæ¯ç§é¢˜å‹çš„ä¸€ä¸ªæ ·æœ¬
    shown_count = 0
    for qtype, questions in list(by_type.items())[:3]:  # åªæ˜¾ç¤ºå‰3ç§é¢˜å‹
        if shown_count >= 3:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ ·æœ¬
            break
            
        sample = random.choice(questions)
        confidence = sample.final_result.confidence
        status = "âœ…" if confidence >= 0.7 else "âš ï¸" if confidence >= 0.5 else "âŒ"
        
        print(f"  {status} {qtype} (ç½®ä¿¡åº¦: {confidence:.3f}):")
        print(f"     {sample.question.question[:60]}...")
        
        if sample.question.options:
            print(f"     é€‰é¡¹æ•°: {len(sample.question.options)}")
        
        if sample.question.answer_raw:
            print(f"     ç­”æ¡ˆ: {sample.question.answer_raw[:20]}...")
        
        shown_count += 1


def calculate_quality_score(stats):
    """è®¡ç®—è´¨é‡è¯„åˆ†"""
    total = stats["total_questions"]
    
    # å„é¡¹æŒ‡æ ‡æƒé‡
    high_conf_rate = stats["confidence_levels"]["high"] / total
    unknown_rate = stats["type_distribution"].get("unknown", 0) / total
    review_rate = stats["needs_review"] / total
    
    # è¯„åˆ†é€»è¾‘
    score = 0
    
    # é«˜ç½®ä¿¡åº¦æ¯”ä¾‹ (40åˆ†)
    if high_conf_rate >= 0.8:
        score += 40
    elif high_conf_rate >= 0.6:
        score += 30
    elif high_conf_rate >= 0.4:
        score += 20
    else:
        score += 10
    
    # æœªçŸ¥é¢˜å‹æ¯”ä¾‹ (30åˆ†)
    if unknown_rate <= 0.1:
        score += 30
    elif unknown_rate <= 0.2:
        score += 20
    elif unknown_rate <= 0.3:
        score += 10
    
    # éœ€å¤æ ¸æ¯”ä¾‹ (30åˆ†)
    if review_rate <= 0.1:
        score += 30
    elif review_rate <= 0.2:
        score += 20
    elif review_rate <= 0.3:
        score += 10
    
    return score


def generate_summary_report(reports):
    """ç”Ÿæˆæ€»ä½“æŠ¥å‘Š"""
    total_files = len(reports)
    total_questions = sum(r["total_questions"] for r in reports)
    
    print(f"ğŸ“Š å¤„ç†æ¦‚è§ˆ:")
    print(f"  é¢˜åº“æ–‡ä»¶æ•°: {total_files}")
    print(f"  é¢˜ç›®æ€»æ•°: {total_questions}")
    
    # è´¨é‡è¯„åˆ†åˆ†å¸ƒ
    scores = [r["quality_score"] for r in reports]
    avg_score = sum(scores) / len(scores) if scores else 0
    
    print(f"\nğŸ¯ è´¨é‡è¯„åˆ†:")
    print(f"  å¹³å‡åˆ†: {avg_score:.1f}/100")
    
    excellent = sum(1 for s in scores if s >= 80)
    good = sum(1 for s in scores if 60 <= s < 80)
    fair = sum(1 for s in scores if 40 <= s < 60)
    poor = sum(1 for s in scores if s < 40)
    
    print(f"  ä¼˜ç§€ (â‰¥80åˆ†): {excellent} ä¸ª")
    print(f"  è‰¯å¥½ (60-79åˆ†): {good} ä¸ª")
    print(f"  ä¸€èˆ¬ (40-59åˆ†): {fair} ä¸ª")
    print(f"  è¾ƒå·® (<40åˆ†): {poor} ä¸ª")
    
    # æ¨èä½¿ç”¨çš„é¢˜åº“
    print(f"\nğŸ“š æ¨èé¢˜åº“:")
    good_files = [r for r in reports if r["quality_score"] >= 70]
    good_files.sort(key=lambda x: x["quality_score"], reverse=True)
    
    for i, report in enumerate(good_files[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"  {i+1}. {report['filename']} (è¯„åˆ†: {report['quality_score']}/100)")
    
    if not good_files:
        print("  âš ï¸  æš‚æ— é«˜è´¨é‡é¢˜åº“ï¼Œå»ºè®®ä¼˜åŒ–åä½¿ç”¨")
    
    # æ€»ä½“å»ºè®®
    print(f"\nğŸ’¡ æ€»ä½“å»ºè®®:")
    if avg_score >= 70:
        print("  âœ… é¢˜åº“è´¨é‡æ•´ä½“è‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨")
    elif avg_score >= 50:
        print("  âš ï¸  é¢˜åº“è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®é€‰æ‹©æ€§ä½¿ç”¨é«˜åˆ†é¢˜åº“")
    else:
        print("  âŒ é¢˜åº“è´¨é‡åä½ï¼Œå»ºè®®è¿›è¡Œä¼˜åŒ–")
        print("  ğŸ“ å¯è€ƒè™‘ï¼š")
        print("    - æ ‡æ³¨éƒ¨åˆ†æ•°æ®è®­ç»ƒXGBoostæ¨¡å‹")
        print("    - ä¼˜åŒ–é¢˜ç›®æ ¼å¼å’Œè§„åˆ™å¼•æ“")
        print("    - è¿›è¡Œæ¦‚ç‡æ ¡å‡†æé«˜ç½®ä¿¡åº¦")


if __name__ == "__main__":
    quick_validate_all()
