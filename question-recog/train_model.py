#!/usr/bin/env python
"""
XGBoostæ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import sys
import json
import jsonlines
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import xgboost as xgb
from collections import Counter

# æ·»åŠ srcåˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.features.extractor import FeatureExtractor
from src.io.schemas import ParsedQuestion


def prepare_training_data():
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print("ğŸ”„ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶ - ä¼˜å…ˆä½¿ç”¨æ‰©å±•æ•°æ®
    expanded_file = Path("data/labels/expanded_labels.jsonl")
    labels_file = Path("data/labels/manual_labels.jsonl")
    
    if expanded_file.exists():
        labels_file = expanded_file
        print(f"âœ… ä½¿ç”¨æ‰©å±•æ•°æ®é›†: {labels_file}")
    elif labels_file.exists():
        print(f"âœ… ä½¿ç”¨æ‰‹åŠ¨æ ‡æ³¨æ•°æ®: {labels_file}")
    else:
        print("âŒ æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ expand_data.py æˆ– annotation_tool.py")
        return None, None, None
    
    # æ£€æŸ¥ç”Ÿäº§ç»“æœæ–‡ä»¶
    results_file = Path("production_results.json")
    if not results_file.exists():
        print("âŒ ç”Ÿäº§ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ production_test.py")
        return None, None, None
    
    # åŠ è½½æ ‡æ³¨æ•°æ®
    labels = {}
    with jsonlines.open(labels_file) as reader:
        for item in reader:
            labels[item["source_id"]] = item["gold_type"]
    
    print(f"ğŸ“Š åŠ è½½äº† {len(labels)} ä¸ªæ ‡æ³¨æ ·æœ¬")
    
    # åŠ è½½ç”Ÿäº§ç»“æœ
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = data["results"]
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = FeatureExtractor()
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X = []
    y = []
    feature_names = extractor.get_feature_names()
    
    labeled_count = 0
    for result in results:
        source_id = result["source_id"]
        
        if source_id in labels:
            # æœ‰æ ‡æ³¨çš„æ ·æœ¬
            gold_type = labels[source_id]
            features = result["features"]
            
            # è½¬æ¢ä¸ºç‰¹å¾å‘é‡
            feature_vector = [features[name] for name in feature_names]
            X.append(feature_vector)
            y.append(gold_type)
            labeled_count += 1
    
    print(f"âœ… å‡†å¤‡äº† {labeled_count} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    if labeled_count < 10:
        print("âŒ è®­ç»ƒæ ·æœ¬å¤ªå°‘ï¼Œéœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬")
        return None, None, None
    
    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    label_counts = Counter(y)
    print(f"ğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in label_counts.items():
        print(f"  {label:15}: {count:3} æ ·æœ¬")
    
    # æ ‡ç­¾ç¼–ç  - å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—
    from sklearn.preprocessing import LabelEncoder
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
    import joblib
    model_dir = Path("src/model")
    model_dir.mkdir(parents=True, exist_ok=True)
    joblib.dump(label_encoder, model_dir / "label_encoder.pkl")
    
    return np.array(X), y_encoded, feature_names, label_encoder.classes_


def train_xgboost_model():
    """è®­ç»ƒXGBoostæ¨¡å‹"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹")
    print("=" * 40)
    
    # å‡†å¤‡æ•°æ®
    result = prepare_training_data()
    if result is None:
        return
    
    X, y, feature_names, class_names = result
    
    # æ•°æ®åˆ†å‰² - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    test_size = min(0.3, max(0.1, len(X) // 10))  # åŠ¨æ€è°ƒæ•´æµ‹è¯•é›†å¤§å°
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›è¡Œåˆ†å±‚æŠ½æ ·
    from collections import Counter
    label_counts = Counter(y)
    min_count = min(label_counts.values())
    
    if min_count >= 2:
        # å¯ä»¥è¿›è¡Œåˆ†å±‚æŠ½æ ·
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
    else:
        # ä¸èƒ½åˆ†å±‚æŠ½æ ·ï¼Œä½¿ç”¨ç®€å•éšæœºåˆ†å‰²
        print(f"âš ï¸  æ•°æ®ä¸å¹³è¡¡ï¼Œä½¿ç”¨ç®€å•éšæœºåˆ†å‰² (æœ€å°‘ç±»åˆ«åªæœ‰{min_count}ä¸ªæ ·æœ¬)")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"ğŸ“Š æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    # åˆ›å»ºXGBoostæ¨¡å‹
    model = xgb.XGBClassifier(
        max_depth=4,           # é™ä½å¤æ‚åº¦é˜²æ­¢è¿‡æ‹Ÿåˆ
        n_estimators=100,      # å‡å°‘æ ‘çš„æ•°é‡
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='mlogloss',
        use_label_encoder=False
    )
    
    # äº¤å‰éªŒè¯
    if len(X_train) >= 5:  # è‡³å°‘5ä¸ªæ ·æœ¬æ‰åšäº¤å‰éªŒè¯
        cv_folds = min(3, len(set(y_train)))  # åŠ¨æ€è°ƒæ•´æŠ˜æ•°
        cv_scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring='f1_macro')
        print(f"ğŸ¯ äº¤å‰éªŒè¯ F1 åˆ†æ•°: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸ”„ è®­ç»ƒæ¨¡å‹ä¸­...")
    model.fit(X_train, y_train)
    
    # æµ‹è¯•é›†è¯„ä¼°
    y_pred = model.predict(X_test)
    
    # å°†æ•°å­—æ ‡ç­¾è½¬æ¢å›å­—ç¬¦ä¸²ç”¨äºæŠ¥å‘Š
    from sklearn.preprocessing import LabelEncoder
    import joblib
    label_encoder = joblib.load("src/model/label_encoder.pkl")
    y_test_str = label_encoder.inverse_transform(y_test)
    y_pred_str = label_encoder.inverse_transform(y_pred)
    
    f1_macro = f1_score(y_test, y_pred, average='macro')
    f1_weighted = f1_score(y_test, y_pred, average='weighted')
    
    print(f"âœ… æµ‹è¯•é›† F1 åˆ†æ•° (å®å¹³å‡): {f1_macro:.4f}")
    print(f"âœ… æµ‹è¯•é›† F1 åˆ†æ•° (åŠ æƒå¹³å‡): {f1_weighted:.4f}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    report = classification_report(y_test_str, y_pred_str, zero_division=0)
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(report)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test_str, y_pred_str, labels=class_names)
    print(f"\nğŸ” æ··æ·†çŸ©é˜µ:")
    print(cm)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = model.feature_importances_
    important_features = sorted(
        zip(feature_names, feature_importance), 
        key=lambda x: x[1], reverse=True
    )[:10]
    
    print(f"\nğŸ¯ å‰10ä¸ªé‡è¦ç‰¹å¾:")
    for feature, importance in important_features:
        print(f"  {feature:20}: {importance:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    model_dir = Path("src/model")
    model_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = model_dir / "xgb_model.json"
    model.save_model(str(model_path))
    
    # ä¿å­˜ç‰¹å¾åç§°
    feature_info_path = model_dir / "feature_names.json"
    with open(feature_info_path, 'w', encoding='utf-8') as f:
        json.dump(feature_names, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    training_info = {
        "model_type": "xgboost",
        "training_samples": len(X_train),
        "test_samples": len(X_test),
        "f1_macro": float(f1_macro),
        "f1_weighted": float(f1_weighted),
        "feature_count": len(feature_names),
        "classes": list(class_names),
        "feature_importance": {k: float(v) for k, v in important_features}  # è½¬æ¢ä¸ºfloat
    }
    
    info_path = model_dir / "training_info.json"
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(training_info, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    print(f"ğŸ’¾ è®­ç»ƒä¿¡æ¯å·²ä¿å­˜è‡³: {info_path}")
    
    # ç»™å‡ºå»ºè®®
    print(f"\nğŸ’¡ è®­ç»ƒå»ºè®®:")
    if f1_macro < 0.7:
        print(f"  â€¢ F1åˆ†æ•°è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ›´å¤šæ ‡æ³¨æ•°æ®")
    if len(X_train) < 50:
        print(f"  â€¢ è®­ç»ƒæ ·æœ¬è¾ƒå°‘ï¼Œå»ºè®®æ ‡æ³¨æ›´å¤šæ•°æ®ä»¥æé«˜æ€§èƒ½")
    
    print(f"\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ¯ ä¸‹ä¸€æ­¥: è¿è¡Œ test_trained_model.py æµ‹è¯•æ¨¡å‹æ•ˆæœ")


if __name__ == "__main__":
    train_xgboost_model()
