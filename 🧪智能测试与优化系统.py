#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æµ‹è¯•ä¸ä¼˜åŒ–ç³»ç»Ÿ - é—­ç¯æ§åˆ¶ç‰ˆæœ¬
å®ç°è‡ªåŠ¨æ§åˆ¶ä¸­çš„é—­ç¯æ§åˆ¶ï¼šåŸå› â†’ç»“æœâ†’åé¦ˆâ†’ä¼˜åŒ–â†’åŸå› 

æµ‹è¯•ç›®æ ‡ï¼š
1. é¢˜ç›®è¯»å–å‡†ç¡®ç‡æµ‹è¯•
2. é¢˜å¹²ä¸é€‰é¡¹è¯†åˆ«å‡†ç¡®ç‡æµ‹è¯•  
3. é¢˜å‹è¯†åˆ«å‡†ç¡®ç‡æµ‹è¯•
4. è‡ªåŠ¨ä¼˜åŒ–å’Œåé¦ˆæœºåˆ¶
"""

import os
import json
import time
import random
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
import traceback
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns

class æ™ºèƒ½æµ‹è¯•ä¸ä¼˜åŒ–ç³»ç»Ÿ:
    """æ™ºèƒ½æµ‹è¯•ä¸ä¼˜åŒ–ç³»ç»Ÿ - å®ç°é—­ç¯æ§åˆ¶"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_data_dir = self.project_root / '.test_data'
        self.test_data_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.test_results = {}
        self.optimization_history = []
        self.performance_metrics = defaultdict(list)
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            'sample_size': 100,  # æµ‹è¯•æ ·æœ¬å¤§å°
            'confidence_threshold': 0.8,  # ç½®ä¿¡åº¦é˜ˆå€¼
            'optimization_iterations': 5,  # ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
            'performance_target': 0.95  # æ€§èƒ½ç›®æ ‡
        }
        
        print("ğŸ§ª æ™ºèƒ½æµ‹è¯•ä¸ä¼˜åŒ–ç³»ç»Ÿåˆå§‹åŒ–")
        print("ğŸ¯ å®ç°é—­ç¯æ§åˆ¶ï¼šæµ‹è¯•â†’åˆ†æâ†’ä¼˜åŒ–â†’åé¦ˆâ†’å†æµ‹è¯•")
        print("=" * 60)
    
    def è¿è¡Œå®Œæ•´æµ‹è¯•é—­ç¯(self):
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•ä¼˜åŒ–é—­ç¯"""
        print("ğŸš€ å¯åŠ¨å®Œæ•´æµ‹è¯•ä¼˜åŒ–é—­ç¯")
        
        for iteration in range(self.test_config['optimization_iterations']):
            print(f"\nğŸ”„ ç¬¬ {iteration + 1} è½®ä¼˜åŒ–è¿­ä»£")
            print("-" * 50)
            
            # 1. æµ‹è¯•é˜¶æ®µ
            test_results = self.æ‰§è¡Œå…¨é¢æµ‹è¯•()
            
            # 2. åˆ†æé˜¶æ®µ
            analysis_results = self.åˆ†ææµ‹è¯•ç»“æœ(test_results)
            
            # 3. ä¼˜åŒ–é˜¶æ®µ
            optimization_results = self.æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–(analysis_results)
            
            # 4. åé¦ˆé˜¶æ®µ
            feedback_results = self.ç”Ÿæˆåé¦ˆæŠ¥å‘Š(test_results, analysis_results, optimization_results)
            
            # 5. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if self.æ£€æŸ¥ä¼˜åŒ–ç›®æ ‡(feedback_results):
                print(f"ğŸ‰ ç¬¬ {iteration + 1} è½®è¾¾åˆ°ä¼˜åŒ–ç›®æ ‡ï¼")
                break
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š()
    
    def æ‰§è¡Œå…¨é¢æµ‹è¯•(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢çš„ç³»ç»Ÿæµ‹è¯•"""
        print("ğŸ” æ‰§è¡Œå…¨é¢ç³»ç»Ÿæµ‹è¯•...")
        
        test_results = {
            'timestamp': datetime.now().isoformat(),
            'reading_accuracy': self.æµ‹è¯•é¢˜ç›®è¯»å–å‡†ç¡®ç‡(),
            'parsing_accuracy': self.æµ‹è¯•é¢˜å¹²é€‰é¡¹è¯†åˆ«å‡†ç¡®ç‡(),
            'classification_accuracy': self.æµ‹è¯•é¢˜å‹è¯†åˆ«å‡†ç¡®ç‡(),
            'system_performance': self.æµ‹è¯•ç³»ç»Ÿæ€§èƒ½(),
            'error_analysis': self.åˆ†æé”™è¯¯æ¨¡å¼()
        }
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        self.test_results[datetime.now().strftime('%Y%m%d_%H%M%S')] = test_results
        
        return test_results
    
    def æµ‹è¯•é¢˜ç›®è¯»å–å‡†ç¡®ç‡(self) -> Dict[str, Any]:
        """æµ‹è¯•é¢˜ç›®è¯»å–å‡†ç¡®ç‡"""
        print("ğŸ“š æµ‹è¯•é¢˜ç›®è¯»å–å‡†ç¡®ç‡...")
        
        try:
            from é¢˜åº“ç®¡ç† import TikuManager
            
            manager = TikuManager()
            tiku_list = manager.get_tiku_list()
            
            if not tiku_list:
                return {'accuracy': 0, 'total': 0, 'errors': ['æ— é¢˜åº“æ–‡ä»¶']}
            
            total_questions = 0
            successful_reads = 0
            read_errors = []
            
            for tiku_name, tiku_path in tiku_list[:3]:  # æµ‹è¯•å‰3ä¸ªé¢˜åº“
                try:
                    questions = manager.load_tiku(tiku_name)
                    if questions:
                        total_questions += len(questions)
                        successful_reads += len(questions)
                        print(f"âœ… {tiku_name}: æˆåŠŸè¯»å– {len(questions)} é¢˜")
                    else:
                        read_errors.append(f"{tiku_name}: è¯»å–å¤±è´¥")
                        print(f"âŒ {tiku_name}: è¯»å–å¤±è´¥")
                        
                except Exception as e:
                    read_errors.append(f"{tiku_name}: {str(e)}")
                    print(f"âŒ {tiku_name}: å¼‚å¸¸ - {e}")
            
            accuracy = successful_reads / total_questions if total_questions > 0 else 0
            
            return {
                'accuracy': accuracy,
                'total': total_questions,
                'successful': successful_reads,
                'errors': read_errors,
                'details': f"æˆåŠŸè¯»å– {successful_reads}/{total_questions} é¢˜"
            }
            
        except Exception as e:
            return {'accuracy': 0, 'total': 0, 'errors': [f'æµ‹è¯•å¼‚å¸¸: {e}']}
    
    def æµ‹è¯•é¢˜å¹²é€‰é¡¹è¯†åˆ«å‡†ç¡®ç‡(self) -> Dict[str, Any]:
        """æµ‹è¯•é¢˜å¹²ä¸é€‰é¡¹è¯†åˆ«å‡†ç¡®ç‡"""
        print("ğŸ¯ æµ‹è¯•é¢˜å¹²é€‰é¡¹è¯†åˆ«å‡†ç¡®ç‡...")
        
        try:
            from é¢˜åº“ç®¡ç† import TikuManager
            
            manager = TikuManager()
            tiku_list = manager.get_tiku_list()
            
            if not tiku_list:
                return {'accuracy': 0, 'total': 0, 'errors': ['æ— é¢˜åº“æ–‡ä»¶']}
            
            total_questions = 0
            correct_parsing = 0
            parsing_errors = []
            
            # æµ‹è¯•æ ·æœ¬
            sample_size = min(self.test_config['sample_size'], 50)
            
            for tiku_name, tiku_path in tiku_list[:2]:  # æµ‹è¯•å‰2ä¸ªé¢˜åº“
                try:
                    questions = manager.load_tiku(tiku_name)
                    if not questions:
                        continue
                    
                    # éšæœºé‡‡æ ·
                    sample_questions = random.sample(questions, min(sample_size, len(questions)))
                    
                    for question in sample_questions:
                        total_questions += 1
                        
                        # æ£€æŸ¥é¢˜å¹²å®Œæ•´æ€§
                        if not question.get('question') or len(question['question'].strip()) < 5:
                            parsing_errors.append(f"é¢˜å¹²ä¸å®Œæ•´: {question.get('question', '')[:50]}...")
                            continue
                        
                        # æ£€æŸ¥é€‰é¡¹è§£æ
                        options = question.get('options', {})
                        if options:
                            # æ£€æŸ¥é€‰é¡¹æ ¼å¼
                            valid_options = 0
                            for opt_key, opt_value in options.items():
                                if opt_value and len(opt_value.strip()) > 1:
                                    valid_options += 1
                            
                            if valid_options >= 2:  # è‡³å°‘2ä¸ªæœ‰æ•ˆé€‰é¡¹
                                correct_parsing += 1
                            else:
                                parsing_errors.append(f"é€‰é¡¹è§£æä¸å®Œæ•´: {len(options)}ä¸ªé€‰é¡¹")
                        else:
                            # æ— é€‰é¡¹é¢˜ç›®ï¼ˆå¡«ç©ºé¢˜ã€ç®€ç­”é¢˜ç­‰ï¼‰
                            if question.get('answer') and len(question['answer'].strip()) > 0:
                                correct_parsing += 1
                            else:
                                parsing_errors.append("æ— é€‰é¡¹ä¸”æ— ç­”æ¡ˆ")
                        
                except Exception as e:
                    parsing_errors.append(f"{tiku_name}: {str(e)}")
            
            accuracy = correct_parsing / total_questions if total_questions > 0 else 0
            
            return {
                'accuracy': accuracy,
                'total': total_questions,
                'correct': correct_parsing,
                'errors': parsing_errors[:10],  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                'details': f"æ­£ç¡®è§£æ {correct_parsing}/{total_questions} é¢˜"
            }
            
        except Exception as e:
            return {'accuracy': 0, 'total': 0, 'errors': [f'æµ‹è¯•å¼‚å¸¸: {e}']}
    
    def æµ‹è¯•é¢˜å‹è¯†åˆ«å‡†ç¡®ç‡(self) -> Dict[str, Any]:
        """æµ‹è¯•é¢˜å‹è¯†åˆ«å‡†ç¡®ç‡"""
        print("ğŸ” æµ‹è¯•é¢˜å‹è¯†åˆ«å‡†ç¡®ç‡...")
        
        try:
            # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
            test_cases = self.åˆ›å»ºé¢˜å‹æµ‹è¯•ç”¨ä¾‹()
            
            total_tests = len(test_cases)
            correct_predictions = 0
            prediction_details = []
            
            # æµ‹è¯•ä¸åŒè¯†åˆ«ç³»ç»Ÿ
            recognition_systems = [
                ('æ™ºèƒ½è¯†åˆ«', self.æµ‹è¯•æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ),
                ('é«˜ç²¾åº¦è¯†åˆ«', self.æµ‹è¯•é«˜ç²¾åº¦è¯†åˆ«ç³»ç»Ÿ),
                ('åŒç³»ç»Ÿè¯†åˆ«', self.æµ‹è¯•åŒç³»ç»Ÿè¯†åˆ«)
            ]
            
            system_results = {}
            
            for system_name, test_func in recognition_systems:
                try:
                    system_correct = 0
                    system_details = []
                    
                    for case in test_cases:
                        predicted_type = test_func(case['question'], case['answer'], case.get('options', {}))
                        
                        if predicted_type == case['expected_type']:
                            system_correct += 1
                            system_details.append(f"âœ… {case['name']}: {predicted_type}")
                        else:
                            system_details.append(f"âŒ {case['name']}: æœŸæœ›{case['expected_type']}, å®é™…{predicted_type}")
                    
                    system_accuracy = system_correct / total_tests
                    system_results[system_name] = {
                        'accuracy': system_accuracy,
                        'correct': system_correct,
                        'total': total_tests,
                        'details': system_details[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
                    }
                    
                    print(f"ğŸ“Š {system_name}: {system_accuracy:.1%} ({system_correct}/{total_tests})")
                    
                except Exception as e:
                    system_results[system_name] = {'accuracy': 0, 'error': str(e)}
                    print(f"âŒ {system_name}: æµ‹è¯•å¼‚å¸¸ - {e}")
            
            # é€‰æ‹©æœ€ä½³ç³»ç»Ÿç»“æœ
            best_system = max(system_results.items(), key=lambda x: x[1].get('accuracy', 0))
            
            return {
                'overall_accuracy': best_system[1].get('accuracy', 0),
                'best_system': best_system[0],
                'system_results': system_results,
                'total_tests': total_tests,
                'details': f"æœ€ä½³ç³»ç»Ÿ {best_system[0]}: {best_system[1].get('accuracy', 0):.1%}"
            }
            
        except Exception as e:
            return {'overall_accuracy': 0, 'error': f'æµ‹è¯•å¼‚å¸¸: {e}'}
    
    def åˆ›å»ºé¢˜å‹æµ‹è¯•ç”¨ä¾‹(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºé¢˜å‹è¯†åˆ«æµ‹è¯•ç”¨ä¾‹"""
        return [
            # å•é€‰é¢˜æµ‹è¯•ç”¨ä¾‹
            {
                'name': 'å•é€‰é¢˜-åŸºç¡€',
                'question': 'ä¸‹åˆ—å“ªä¸ªæ˜¯æ­£ç¡®çš„å®‰å…¨æªæ–½ï¼Ÿ',
                'answer': 'A',
                'options': {'A': 'åœç”µ', 'B': 'éªŒç”µ', 'C': 'è£…è®¾æ¥åœ°çº¿', 'D': 'ä»¥ä¸Šéƒ½æ˜¯'},
                'expected_type': 'å•é€‰é¢˜'
            },
            {
                'name': 'å•é€‰é¢˜-å…³é”®è¯',
                'question': 'ç”µåŠ›å®‰å…¨å·¥ä½œä¸­ï¼Œæœ€æ­£ç¡®çš„åšæ³•æ˜¯ï¼š',
                'answer': 'B',
                'options': {'A': 'å•äººä½œä¸š', 'B': 'åŒäººä½œä¸š', 'C': 'æ— ç›‘æŠ¤ä½œä¸š', 'D': 'éšæ„ä½œä¸š'},
                'expected_type': 'å•é€‰é¢˜'
            },
            
            # å¤šé€‰é¢˜æµ‹è¯•ç”¨ä¾‹
            {
                'name': 'å¤šé€‰é¢˜-åŸºç¡€',
                'question': 'ç”µåŠ›å®‰å…¨å·¥ä½œçš„æŠ€æœ¯æªæ–½åŒ…æ‹¬å“ªäº›ï¼Ÿ',
                'answer': 'ABC',
                'options': {'A': 'åœç”µ', 'B': 'éªŒç”µ', 'C': 'è£…è®¾æ¥åœ°çº¿', 'D': 'æ‚¬æŒ‚æ ‡ç¤ºç‰Œ'},
                'expected_type': 'å¤šé€‰é¢˜'
            },
            {
                'name': 'å¤šé€‰é¢˜-å…³é”®è¯',
                'question': 'ä¸‹åˆ—å“ªäº›æ˜¯æ­£ç¡®çš„å®‰å…¨è¦æ±‚ï¼Ÿ',
                'answer': 'ABD',
                'options': {'A': 'æˆ´å®‰å…¨å¸½', 'B': 'ç©¿ç»ç¼˜é‹', 'C': 'ä¸æˆ´æ‰‹å¥—', 'D': 'ä½¿ç”¨å·¥å…·'},
                'expected_type': 'å¤šé€‰é¢˜'
            },
            
            # åˆ¤æ–­é¢˜æµ‹è¯•ç”¨ä¾‹
            {
                'name': 'åˆ¤æ–­é¢˜-åŸºç¡€',
                'question': 'è£…è®¾æ¥åœ°çº¿å¯ä»¥å•äººè¿›è¡Œã€‚',
                'answer': 'é”™',
                'options': {},
                'expected_type': 'åˆ¤æ–­é¢˜'
            },
            {
                'name': 'åˆ¤æ–­é¢˜-ç¬¦å·',
                'question': 'ç”µåŠ›è®¾å¤‡æ£€ä¿®æ—¶å¿…é¡»åœç”µã€‚(âˆš)',
                'answer': 'å¯¹',
                'options': {},
                'expected_type': 'åˆ¤æ–­é¢˜'
            },
            
            # å¡«ç©ºé¢˜æµ‹è¯•ç”¨ä¾‹
            {
                'name': 'å¡«ç©ºé¢˜-åŸºç¡€',
                'question': 'ç”µåŠ›å®‰å…¨å·¥ä½œè§„ç¨‹è§„å®šï¼Œåœç”µæ—¶é—´åº”ä¸å°‘äº____åˆ†é’Ÿã€‚',
                'answer': '30',
                'options': {},
                'expected_type': 'å¡«ç©ºé¢˜'
            },
            {
                'name': 'å¡«ç©ºé¢˜-å•ä½',
                'question': 'å®‰å…¨è·ç¦»åº”ä¿æŒ____ç±³ä»¥ä¸Šã€‚',
                'answer': '1.5',
                'options': {},
                'expected_type': 'å¡«ç©ºé¢˜'
            },
            
            # ç®€ç­”é¢˜æµ‹è¯•ç”¨ä¾‹
            {
                'name': 'ç®€ç­”é¢˜-åŸºç¡€',
                'question': 'ç®€è¿°ç”µåŠ›å®‰å…¨å·¥ä½œçš„åŸºæœ¬è¦æ±‚ã€‚',
                'answer': 'ç”µåŠ›å®‰å…¨å·¥ä½œçš„åŸºæœ¬è¦æ±‚åŒ…æ‹¬ï¼š1.ä¸¥æ ¼æ‰§è¡Œå®‰å…¨è§„ç¨‹ï¼›2.åšå¥½å®‰å…¨é˜²æŠ¤ï¼›3.åŠ å¼ºå®‰å…¨ç›‘æŠ¤ï¼›4.åŠæ—¶å¤„ç†å®‰å…¨éšæ‚£ã€‚',
                'options': {},
                'expected_type': 'ç®€ç­”é¢˜'
            },
            {
                'name': 'ç®€ç­”é¢˜-åˆ†æ',
                'question': 'åˆ†æç”µåŠ›äº‹æ•…çš„ä¸»è¦åŸå› åŠé¢„é˜²æªæ–½ã€‚',
                'answer': 'ç”µåŠ›äº‹æ•…çš„ä¸»è¦åŸå› åŒ…æ‹¬ï¼š1.è¿åå®‰å…¨è§„ç¨‹ï¼›2.è®¾å¤‡ç¼ºé™·ï¼›3.ç®¡ç†ä¸å–„ï¼›4.äººå‘˜ç´ è´¨ä¸é«˜ã€‚é¢„é˜²æªæ–½ï¼š1.åŠ å¼ºåŸ¹è®­ï¼›2.å®Œå–„åˆ¶åº¦ï¼›3.å®šæœŸæ£€æŸ¥ï¼›4.ä¸¥æ ¼ç®¡ç†ã€‚',
                'options': {},
                'expected_type': 'ç®€ç­”é¢˜'
            }
        ]
    
    def æµ‹è¯•æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ(self, question: str, answer: str, options: Dict) -> str:
        """æµ‹è¯•æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ"""
        try:
            from æ™ºèƒ½é¢˜å‹è¯†åˆ« import detect_question_type
            return detect_question_type(question, answer, options)
        except:
            return 'æœªçŸ¥'
    
    def æµ‹è¯•é«˜ç²¾åº¦è¯†åˆ«ç³»ç»Ÿ(self, question: str, answer: str, options: Dict) -> str:
        """æµ‹è¯•é«˜ç²¾åº¦è¯†åˆ«ç³»ç»Ÿ"""
        try:
            from é«˜ç²¾åº¦é¢˜å‹è¯†åˆ« import detect_question_type_fixed
            return detect_question_type_fixed(question, answer, options)
        except:
            return 'æœªçŸ¥'
    
    def æµ‹è¯•åŒç³»ç»Ÿè¯†åˆ«(self, question: str, answer: str, options: Dict) -> str:
        """æµ‹è¯•åŒç³»ç»Ÿè¯†åˆ«"""
        try:
            from åŒç³»ç»Ÿé¢˜å‹è¯†åˆ«å™¨ import detect_question_type_dual
            q_type, confidence = detect_question_type_dual(question, answer, options)
            return q_type
        except:
            return 'æœªçŸ¥'
    
    def æµ‹è¯•ç³»ç»Ÿæ€§èƒ½(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿæ€§èƒ½"""
        print("âš¡ æµ‹è¯•ç³»ç»Ÿæ€§èƒ½...")
        
        try:
            # æ€§èƒ½æµ‹è¯•ç”¨ä¾‹
            test_questions = [
                ("å•é€‰é¢˜æµ‹è¯•", "A", {"A": "é€‰é¡¹A", "B": "é€‰é¡¹B"}),
                ("å¤šé€‰é¢˜æµ‹è¯•", "ABC", {"A": "é€‰é¡¹A", "B": "é€‰é¡¹B", "C": "é€‰é¡¹C"}),
                ("åˆ¤æ–­é¢˜æµ‹è¯•", "å¯¹", {}),
                ("å¡«ç©ºé¢˜æµ‹è¯•", "ç­”æ¡ˆ", {}),
                ("ç®€ç­”é¢˜æµ‹è¯•", "è¿™æ˜¯ä¸€ä¸ªè¯¦ç»†çš„ç­”æ¡ˆè¯´æ˜", {})
            ] * 20  # é‡å¤20æ¬¡
            
            # æµ‹è¯•è¯†åˆ«æ€§èƒ½
            start_time = time.time()
            
            for question, answer, options in test_questions:
                try:
                    from åŒç³»ç»Ÿé¢˜å‹è¯†åˆ«å™¨ import detect_question_type_dual
                    detect_question_type_dual(question, answer, options)
                except:
                    pass
            
            end_time = time.time()
            total_time = end_time - start_time
            avg_time = total_time / len(test_questions)
            throughput = len(test_questions) / total_time
            
            return {
                'total_time': total_time,
                'avg_time_per_question': avg_time,
                'throughput': throughput,
                'total_questions': len(test_questions),
                'performance_level': self.è¯„ä¼°æ€§èƒ½ç­‰çº§(avg_time)
            }
            
        except Exception as e:
            return {'error': f'æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}'}
    
    def è¯„ä¼°æ€§èƒ½ç­‰çº§(self, avg_time: float) -> str:
        """è¯„ä¼°æ€§èƒ½ç­‰çº§"""
        if avg_time < 0.01:
            return 'ä¼˜ç§€ (< 0.01ç§’)'
        elif avg_time < 0.05:
            return 'è‰¯å¥½ (< 0.05ç§’)'
        elif avg_time < 0.1:
            return 'ä¸€èˆ¬ (< 0.1ç§’)'
        else:
            return 'éœ€è¦ä¼˜åŒ– (> 0.1ç§’)'
    
    def åˆ†æé”™è¯¯æ¨¡å¼(self) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        print("ğŸ” åˆ†æé”™è¯¯æ¨¡å¼...")
        
        try:
            # æ”¶é›†é”™è¯¯ä¿¡æ¯
            error_patterns = defaultdict(int)
            error_sources = defaultdict(int)
            
            # ä»æµ‹è¯•ç»“æœä¸­æå–é”™è¯¯
            for test_result in self.test_results.values():
                if 'errors' in test_result.get('reading_accuracy', {}):
                    for error in test_result['reading_accuracy']['errors']:
                        error_patterns[error] += 1
                        error_sources['reading'] += 1
                
                if 'errors' in test_result.get('parsing_accuracy', {}):
                    for error in test_result['parsing_accuracy']['errors']:
                        error_patterns[error] += 1
                        error_sources['parsing'] += 1
            
            return {
                'error_patterns': dict(error_patterns),
                'error_sources': dict(error_sources),
                'total_errors': sum(error_patterns.values()),
                'most_common_error': max(error_patterns.items(), key=lambda x: x[1]) if error_patterns else None
            }
            
        except Exception as e:
            return {'error': f'é”™è¯¯åˆ†æå¼‚å¸¸: {e}'}
    
    def åˆ†ææµ‹è¯•ç»“æœ(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print("ğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
        
        analysis = {
            'overall_score': 0,
            'component_scores': {},
            'improvement_areas': [],
            'strengths': [],
            'recommendations': []
        }
        
        # è®¡ç®—å„ç»„ä»¶å¾—åˆ†
        reading_acc = test_results.get('reading_accuracy', {}).get('accuracy', 0)
        parsing_acc = test_results.get('parsing_accuracy', {}).get('accuracy', 0)
        classification_acc = test_results.get('classification_accuracy', {}).get('overall_accuracy', 0)
        performance_level = test_results.get('system_performance', {}).get('performance_level', 'éœ€è¦ä¼˜åŒ–')
        
        analysis['component_scores'] = {
            'reading_accuracy': reading_acc,
            'parsing_accuracy': parsing_acc,
            'classification_accuracy': classification_acc,
            'performance': 0.9 if 'ä¼˜ç§€' in performance_level else 0.7 if 'è‰¯å¥½' in performance_level else 0.5
        }
        
        # è®¡ç®—æ€»ä½“å¾—åˆ†
        analysis['overall_score'] = sum(analysis['component_scores'].values()) / len(analysis['component_scores'])
        
        # è¯†åˆ«æ”¹è¿›é¢†åŸŸ
        if reading_acc < 0.9:
            analysis['improvement_areas'].append('é¢˜ç›®è¯»å–å‡†ç¡®ç‡')
        if parsing_acc < 0.9:
            analysis['improvement_areas'].append('é¢˜å¹²é€‰é¡¹è¯†åˆ«å‡†ç¡®ç‡')
        if classification_acc < 0.9:
            analysis['improvement_areas'].append('é¢˜å‹è¯†åˆ«å‡†ç¡®ç‡')
        if 'éœ€è¦ä¼˜åŒ–' in performance_level:
            analysis['improvement_areas'].append('ç³»ç»Ÿæ€§èƒ½')
        
        # è¯†åˆ«ä¼˜åŠ¿
        if reading_acc >= 0.95:
            analysis['strengths'].append('é¢˜ç›®è¯»å–ç¨³å®š')
        if parsing_acc >= 0.95:
            analysis['strengths'].append('é¢˜å¹²è§£æå‡†ç¡®')
        if classification_acc >= 0.95:
            analysis['strengths'].append('é¢˜å‹è¯†åˆ«ç²¾ç¡®')
        
        # ç”Ÿæˆå»ºè®®
        if reading_acc < 0.8:
            analysis['recommendations'].append('ä¼˜åŒ–é¢˜åº“æ–‡ä»¶æ ¼å¼ï¼Œæ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§')
        if parsing_acc < 0.8:
            analysis['recommendations'].append('æ”¹è¿›é€‰é¡¹è§£æç®—æ³•ï¼Œå¢å¼ºæ ¼å¼å…¼å®¹æ€§')
        if classification_acc < 0.8:
            analysis['recommendations'].append('è°ƒæ•´é¢˜å‹è¯†åˆ«å‚æ•°ï¼Œå¢åŠ è®­ç»ƒæ ·æœ¬')
        
        return analysis
    
    def æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–"""
        print("ğŸ”§ æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–...")
        
        optimization_results = {
            'optimizations_applied': [],
            'parameters_adjusted': {},
            'success': False
        }
        
        try:
            # æ ¹æ®åˆ†æç»“æœæ‰§è¡Œä¼˜åŒ–
            improvement_areas = analysis_results.get('improvement_areas', [])
            
            if 'é¢˜ç›®è¯»å–å‡†ç¡®ç‡' in improvement_areas:
                self.ä¼˜åŒ–é¢˜ç›®è¯»å–()
                optimization_results['optimizations_applied'].append('é¢˜ç›®è¯»å–ä¼˜åŒ–')
            
            if 'é¢˜å¹²é€‰é¡¹è¯†åˆ«å‡†ç¡®ç‡' in improvement_areas:
                self.ä¼˜åŒ–é¢˜å¹²è§£æ()
                optimization_results['optimizations_applied'].append('é¢˜å¹²è§£æä¼˜åŒ–')
            
            if 'é¢˜å‹è¯†åˆ«å‡†ç¡®ç‡' in improvement_areas:
                self.ä¼˜åŒ–é¢˜å‹è¯†åˆ«()
                optimization_results['optimizations_applied'].append('é¢˜å‹è¯†åˆ«ä¼˜åŒ–')
            
            if 'ç³»ç»Ÿæ€§èƒ½' in improvement_areas:
                self.ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½()
                optimization_results['optimizations_applied'].append('ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–')
            
            optimization_results['success'] = len(optimization_results['optimizations_applied']) > 0
            
        except Exception as e:
            optimization_results['error'] = f'ä¼˜åŒ–å¼‚å¸¸: {e}'
        
        return optimization_results
    
    def ä¼˜åŒ–é¢˜ç›®è¯»å–(self):
        """ä¼˜åŒ–é¢˜ç›®è¯»å–"""
        print("ğŸ”§ ä¼˜åŒ–é¢˜ç›®è¯»å–...")
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„ä¼˜åŒ–é€»è¾‘
        # ä¾‹å¦‚ï¼šè°ƒæ•´è§£æå‚æ•°ã€å¢åŠ é”™è¯¯å¤„ç†ç­‰
        pass
    
    def ä¼˜åŒ–é¢˜å¹²è§£æ(self):
        """ä¼˜åŒ–é¢˜å¹²è§£æ"""
        print("ğŸ”§ ä¼˜åŒ–é¢˜å¹²è§£æ...")
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„ä¼˜åŒ–é€»è¾‘
        # ä¾‹å¦‚ï¼šæ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼ã€å¢å¼ºæ ¼å¼è¯†åˆ«ç­‰
        pass
    
    def ä¼˜åŒ–é¢˜å‹è¯†åˆ«(self):
        """ä¼˜åŒ–é¢˜å‹è¯†åˆ«"""
        print("ğŸ”§ ä¼˜åŒ–é¢˜å‹è¯†åˆ«...")
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„ä¼˜åŒ–é€»è¾‘
        # ä¾‹å¦‚ï¼šè°ƒæ•´è¯†åˆ«é˜ˆå€¼ã€å¢åŠ ç‰¹å¾æƒé‡ç­‰
        pass
    
    def ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½(self):
        """ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½"""
        print("ğŸ”§ ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½...")
        # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„ä¼˜åŒ–é€»è¾‘
        # ä¾‹å¦‚ï¼šç¼“å­˜ä¼˜åŒ–ã€ç®—æ³•ä¼˜åŒ–ç­‰
        pass
    
    def ç”Ÿæˆåé¦ˆæŠ¥å‘Š(self, test_results: Dict, analysis_results: Dict, optimization_results: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆåé¦ˆæŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆåé¦ˆæŠ¥å‘Š...")
        
        feedback = {
            'timestamp': datetime.now().isoformat(),
            'test_summary': {
                'reading_accuracy': test_results.get('reading_accuracy', {}).get('accuracy', 0),
                'parsing_accuracy': test_results.get('parsing_accuracy', {}).get('accuracy', 0),
                'classification_accuracy': test_results.get('classification_accuracy', {}).get('overall_accuracy', 0),
                'overall_score': analysis_results.get('overall_score', 0)
            },
            'optimization_summary': {
                'optimizations_applied': optimization_results.get('optimizations_applied', []),
                'success': optimization_results.get('success', False)
            },
            'recommendations': analysis_results.get('recommendations', []),
            'next_steps': self.ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè®®(analysis_results, optimization_results)
        }
        
        # ä¿å­˜åé¦ˆæŠ¥å‘Š
        feedback_file = self.test_data_dir / f'feedback_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(feedback_file, 'w', encoding='utf-8') as f:
            json.dump(feedback, f, ensure_ascii=False, indent=2)
        
        return feedback
    
    def ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè®®(self, analysis_results: Dict, optimization_results: Dict) -> List[str]:
        """ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè®®"""
        suggestions = []
        
        overall_score = analysis_results.get('overall_score', 0)
        
        if overall_score < 0.7:
            suggestions.append('ç³»ç»Ÿéœ€è¦é‡å¤§æ”¹è¿›ï¼Œå»ºè®®é‡æ–°è®¾è®¡æ ¸å¿ƒç®—æ³•')
        elif overall_score < 0.85:
            suggestions.append('ç³»ç»Ÿéœ€è¦æŒç»­ä¼˜åŒ–ï¼Œé‡ç‚¹å…³æ³¨è–„å¼±ç¯èŠ‚')
        elif overall_score < 0.95:
            suggestions.append('ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œè¿›è¡Œç²¾ç»†åŒ–è°ƒä¼˜')
        else:
            suggestions.append('ç³»ç»Ÿæ€§èƒ½ä¼˜ç§€ï¼Œä¿æŒå½“å‰çŠ¶æ€')
        
        if not optimization_results.get('success', False):
            suggestions.append('ä¼˜åŒ–æœªç”Ÿæ•ˆï¼Œéœ€è¦æ£€æŸ¥ä¼˜åŒ–ç­–ç•¥')
        
        return suggestions
    
    def æ£€æŸ¥ä¼˜åŒ–ç›®æ ‡(self, feedback_results: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¼˜åŒ–ç›®æ ‡"""
        overall_score = feedback_results.get('test_summary', {}).get('overall_score', 0)
        target_score = self.test_config['performance_target']
        
        return overall_score >= target_score
    
    def ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š(self):
        """ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•æ•°æ®
        all_results = list(self.test_results.values())
        
        if not all_results:
            print("âŒ æ— æµ‹è¯•æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return
        
        # è®¡ç®—è¶‹åŠ¿
        reading_trend = [r.get('reading_accuracy', {}).get('accuracy', 0) for r in all_results]
        parsing_trend = [r.get('parsing_accuracy', {}).get('accuracy', 0) for r in all_results]
        classification_trend = [r.get('classification_accuracy', {}).get('overall_accuracy', 0) for r in all_results]
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'summary': {
                'total_iterations': len(all_results),
                'final_reading_accuracy': reading_trend[-1] if reading_trend else 0,
                'final_parsing_accuracy': parsing_trend[-1] if parsing_trend else 0,
                'final_classification_accuracy': classification_trend[-1] if classification_trend else 0,
                'improvement_achieved': self.è®¡ç®—æ”¹è¿›ç¨‹åº¦(reading_trend, parsing_trend, classification_trend)
            },
            'trends': {
                'reading_accuracy': reading_trend,
                'parsing_accuracy': parsing_trend,
                'classification_accuracy': classification_trend
            },
            'recommendations': self.ç”Ÿæˆæœ€ç»ˆå»ºè®®(all_results),
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.test_data_dir / 'final_optimization_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        self.æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦(report)
    
    def è®¡ç®—æ”¹è¿›ç¨‹åº¦(self, reading_trend: List[float], parsing_trend: List[float], classification_trend: List[float]) -> Dict[str, float]:
        """è®¡ç®—æ”¹è¿›ç¨‹åº¦"""
        improvements = {}
        
        if len(reading_trend) > 1:
            improvements['reading'] = reading_trend[-1] - reading_trend[0]
        
        if len(parsing_trend) > 1:
            improvements['parsing'] = parsing_trend[-1] - parsing_trend[0]
        
        if len(classification_trend) > 1:
            improvements['classification'] = classification_trend[-1] - classification_trend[0]
        
        return improvements
    
    def ç”Ÿæˆæœ€ç»ˆå»ºè®®(self, all_results: List[Dict]) -> List[str]:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        suggestions = []
        
        # åŸºäºæ‰€æœ‰æµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        latest_result = all_results[-1] if all_results else {}
        
        reading_acc = latest_result.get('reading_accuracy', {}).get('accuracy', 0)
        parsing_acc = latest_result.get('parsing_accuracy', {}).get('accuracy', 0)
        classification_acc = latest_result.get('classification_accuracy', {}).get('overall_accuracy', 0)
        
        if reading_acc < 0.9:
            suggestions.append('å»ºè®®ä¼˜åŒ–é¢˜åº“æ–‡ä»¶æ ¼å¼ï¼Œæé«˜è¯»å–ç¨³å®šæ€§')
        
        if parsing_acc < 0.9:
            suggestions.append('å»ºè®®æ”¹è¿›é¢˜å¹²è§£æç®—æ³•ï¼Œå¢å¼ºæ ¼å¼å…¼å®¹æ€§')
        
        if classification_acc < 0.9:
            suggestions.append('å»ºè®®è°ƒæ•´é¢˜å‹è¯†åˆ«å‚æ•°ï¼Œå¢åŠ è®­ç»ƒæ ·æœ¬')
        
        suggestions.append('å»ºè®®å®šæœŸè¿è¡Œæµ‹è¯•é—­ç¯ï¼ŒæŒç»­ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½')
        
        return suggestions
    
    def æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦(self, report: Dict[str, Any]):
        """æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ‰ æœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š")
        print("=" * 60)
        
        summary = report['summary']
        print(f"ğŸ“Š æµ‹è¯•è¿­ä»£æ¬¡æ•°: {summary['total_iterations']}")
        print(f"ğŸ“š æœ€ç»ˆè¯»å–å‡†ç¡®ç‡: {summary['final_reading_accuracy']:.1%}")
        print(f"ğŸ¯ æœ€ç»ˆè§£æå‡†ç¡®ç‡: {summary['final_parsing_accuracy']:.1%}")
        print(f"ğŸ” æœ€ç»ˆè¯†åˆ«å‡†ç¡®ç‡: {summary['final_classification_accuracy']:.1%}")
        
        improvements = summary.get('improvement_achieved', {})
        if improvements:
            print(f"\nğŸ“ˆ æ”¹è¿›ç¨‹åº¦:")
            for component, improvement in improvements.items():
                print(f"  {component}: {improvement:+.1%}")
        
        print(f"\nğŸ’¡ å»ºè®®:")
        for suggestion in report['recommendations']:
            print(f"  - {suggestion}")
        
        print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {self.test_data_dir / 'final_optimization_report.json'}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½æµ‹è¯•ä¸ä¼˜åŒ–ç³»ç»Ÿ")
    print("ğŸ¯ å®ç°é—­ç¯æ§åˆ¶ï¼šæµ‹è¯•â†’åˆ†æâ†’ä¼˜åŒ–â†’åé¦ˆâ†’å†æµ‹è¯•")
    print("â° å¼€å§‹æ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    print()
    
    # åˆ›å»ºæµ‹è¯•ç³»ç»Ÿ
    test_system = æ™ºèƒ½æµ‹è¯•ä¸ä¼˜åŒ–ç³»ç»Ÿ()
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•é—­ç¯
    test_system.è¿è¡Œå®Œæ•´æµ‹è¯•é—­ç¯()
    
    print("\nğŸŠ æ™ºèƒ½æµ‹è¯•ä¸ä¼˜åŒ–ç³»ç»Ÿè¿è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()
